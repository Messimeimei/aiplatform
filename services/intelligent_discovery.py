# -*- coding: utf-8 -*-
"""
æ™ºèƒ½å‘ç°æ ¸å¿ƒæ¨¡å—ï¼ˆAIå…³é”®æŠ€æœ¯ä¸äº§å“è¯†åˆ«ï¼‰
----------------------------------------
åŠŸèƒ½ï¼š
1ï¸âƒ£ è‡ªåŠ¨è¯†åˆ«ä¼ å…¥æ–‡æœ¬æˆ–æ–‡ä»¶å†…å®¹
2ï¸âƒ£ è°ƒç”¨ Qwen å¤§æ¨¡å‹æŠ½å–æŠ€æœ¯è¯ / äº§å“è¯
3ï¸âƒ£ åŒ¹é…æœ¬åœ°æ ‡å‡†è¯åº“ï¼ˆrank_table_*.jsonï¼‰
4ï¸âƒ£ è¿”å›ç»“æ„åŒ–ç»“æœ

ä½œè€…ï¼šSeren
ç‰ˆæœ¬ï¼šv2ï¼ˆæ”¹è¿›ç‰ˆï¼šå¼ºå¥è§£æ + æ—¥å¿—æ‰“å° + å®¹é”™æœºåˆ¶ï¼‰
"""

import os
import re
import json
import pdfplumber
from docx import Document
from dashscope import Generation
from dotenv import load_dotenv

# ä¿è¯æ— è®ºåœ¨å“ªè¿è¡Œéƒ½èƒ½æ‰¾åˆ° /app/data
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(os.path.dirname(BASE_DIR), "data")

# ==================== ç¯å¢ƒé…ç½® ====================
load_dotenv()  # è‡ªåŠ¨è¯»å– .env æ–‡ä»¶
API_KEY = os.getenv("DASHSCOPE_API_KEY", "sk-xxxx")  # å»ºè®®æ”¾åœ¨ç¯å¢ƒå˜é‡

# ==================== æ¨¡å‹è°ƒç”¨å‡½æ•° ====================
def call_with_messages_qwen_plus(system_prompt, prompt_text):
    """è°ƒç”¨ Qwen æ¨¡å‹"""
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt_text.strip()},
    ]
    try:
        response = Generation.call(
            api_key=API_KEY,
            model="qwen3-235b-a22b-instruct-2507",
            messages=messages,
            result_format="message",
            temperature=0.7,
            top_p=0.8,
        )
        if response.status_code != 200:
            raise Exception(f"Request failed: {response.status_code}, {response.message}")
        return response.output.choices[0].message.content.strip()
    except Exception as e:
        print("âŒ è°ƒç”¨æ¨¡å‹å¤±è´¥:", e)
        return "{}"

# ==================== æ–‡ä»¶è¯»å–å‡½æ•° ====================
def extract_text_from_file(file_path):
    ext = file_path.lower().split(".")[-1]
    if ext == "txt":
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    elif ext == "pdf":
        text = ""
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    elif ext == "docx":
        doc = Document(file_path)
        return "\n".join([p.text for p in doc.paragraphs])
    else:
        raise ValueError(f"Unsupported file type: {ext}")

# ==================== è¯åº“åŠ è½½å‡½æ•° ====================
def load_rank_tables(json_folder):
    """åŠ è½½ rank_table_*.json æ–‡ä»¶"""
    rank_data = []
    for file_name in os.listdir(json_folder):
        if file_name.startswith("rank_table_") and file_name.endswith(".json"):
            file_path = os.path.join(json_folder, file_name)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    rank_data.extend(json.load(f))
            except Exception as e:
                print(f"âš ï¸ è¯åº“æ–‡ä»¶è¯»å–å¤±è´¥ {file_name}: {e}")
    print(f"ğŸ“š è½½å…¥æ ‡å‡†è¯åº“ {len(rank_data)} æ¡")
    return rank_data

# ==================== æ¨¡ç³ŠåŒ¹é…æ ‡å‡†åŒ–å‡½æ•° ====================
def normalize_terms(extracted_words, key_words):
    """æ¨¡ç³ŠåŒ¹é…æ¨¡å‹æŠ½å–ç»“æœä¸æ ‡å‡†è¯åº“"""
    from rapidfuzz import fuzz

    normalized = []
    for word in extracted_words:
        word_low = word.lower()
        best = None
        best_score = 0
        for key in key_words:
            score = fuzz.partial_ratio(word_low, key.lower())
            if score > best_score:
                best_score = score
                best = key
        if best and best_score >= 70:
            normalized.append(best)
        else:
            normalized.append(word)
    return list(dict.fromkeys(normalized))

# ==================== æ ¸å¿ƒå‡½æ•° ====================
def intelligent_discovery(input_text_or_file, json_folder):
    """ä¸»é€»è¾‘ï¼šæ–‡ä»¶æˆ–æ–‡æœ¬è¾“å…¥ â†’ æ¨¡å‹æŠ½å– â†’ åŒ¹é…è¯åº“ â†’ è¿”å›ç»“æœ"""

    # ---------- 1ï¸âƒ£ è¯»å–æ–‡æœ¬ ----------
    if os.path.exists(input_text_or_file):
        content = extract_text_from_file(input_text_or_file)
    else:
        content = input_text_or_file
    print("ğŸ“¥ è¾“å…¥å†…å®¹é•¿åº¦:", len(content))

    # ---------- 2ï¸âƒ£ è°ƒç”¨æ¨¡å‹ ----------
    system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æŠ€æœ¯å‘ç°åŠ©æ‰‹ã€‚
        è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æŠ½å–æŠ€æœ¯è¯ï¼ˆå¦‚æ–¹æ³•ã€ç®—æ³•ã€ææ–™ã€æŠ€æœ¯åç§°ï¼‰å’Œäº§å“è¯ï¼ˆå¦‚è®¾å¤‡ã€å·¥å…·ã€äº§å“å‹å·ï¼‰ã€‚
        è¯·åªè¾“å‡º JSON æ ¼å¼ï¼Œä¸è¦å¤šä½™è§£é‡Šã€‚æ ¼å¼å¦‚ä¸‹ï¼š
        {
          "all_tech_words": ["æŠ€æœ¯1", "æŠ€æœ¯2"],
          "all_product_words": ["äº§å“1", "äº§å“2"]
        }
            """

    raw_output = call_with_messages_qwen_plus(system_prompt, content)
    print("ğŸ§  æ¨¡å‹åŸå§‹è¾“å‡º >>>", raw_output)

    # ---------- 3ï¸âƒ£ è§£ææ¨¡å‹è¿”å› ----------
    extraction_json = {"all_tech_words": [], "all_product_words": []}
    try:
        match = re.search(r"\{[\s\S]*\}", raw_output)
        if match:
            extraction_json = json.loads(match.group(0))
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°JSONç»“æ„ï¼Œä½¿ç”¨ç©ºç»“æ„")
    except Exception as e:
        print("âŒ JSONè§£æå¤±è´¥:", e)
        print("âš ï¸ åŸå§‹è¾“å‡º:", raw_output)

    all_tech_words = extraction_json.get("all_tech_words", [])
    all_product_words = extraction_json.get("all_product_words", [])
    print("ğŸ¯ æŠ½å–ç»“æœï¼š", all_tech_words, all_product_words)

    # ---------- 4ï¸âƒ£ åŠ è½½è¯åº“ ----------
    rank_entries = load_rank_tables(json_folder)
    key_tech_words = [item['name'] for item in rank_entries if item.get("type") == "æŠ€æœ¯"]
    key_product_words = [item['name'] for item in rank_entries if item.get("type") == "äº§å“"]

    # ---------- 5ï¸âƒ£ æ¨¡ç³ŠåŒ¹é… ----------
    normalized_tech = normalize_terms(all_tech_words, key_tech_words)
    normalized_product = normalize_terms(all_product_words, key_product_words)

    key_tech_found = [w for w in normalized_tech if w in key_tech_words]
    key_products_found = [w for w in normalized_product if w in key_product_words]

    # ---------- 6ï¸âƒ£ è¾“å‡º ----------
    print("âœ… åŒ¹é…åˆ°çš„å…³é”®æŠ€æœ¯:", key_tech_found)
    print("âœ… åŒ¹é…åˆ°çš„å…³é”®äº§å“:", key_products_found)

    return {
        "all_tech_words": normalized_tech,
        "all_product_words": normalized_product,
        "key_tech_found": key_tech_found,
        "key_products_found": key_products_found,
    }

# ==================== ç‹¬ç«‹æµ‹è¯•å…¥å£ ====================
if __name__ == "__main__":
    test_text = (
        "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œ(GNN)çš„æ¨èç®—æ³•ï¼Œå¹¶å¼€å‘äº†OCRè¯†åˆ«ç³»ç»Ÿã€‚"
        "æ­¤å¤–è¿˜ä½¿ç”¨å¤šæ¨¡æ€Transformerè¿›è¡Œå›¾åƒ-æ–‡æœ¬è”åˆåˆ†æã€‚"
    )
    res = intelligent_discovery(test_text, "data")
    print(json.dumps(res, ensure_ascii=False, indent=2))
